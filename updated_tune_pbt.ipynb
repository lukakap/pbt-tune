{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cb8f515-add9-4a80-a205-78667468f748",
   "metadata": {},
   "source": [
    "# Population Based Training with RAY TUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c49e26-d7b9-4638-bdc6-b60ecf34372f",
   "metadata": {},
   "source": [
    "Hyperparameter tuning is a key step in model selection. Hyperparameters are like settings, if you do not handle them appropriately, it can have a bad impact on the results of the model. Tuning can be done manually or automatically. In today's world, because of computational capabilities, a high number of hyperparameters, a big variety of algorithms, and helper libraries like the Ray, the preferred way is automatically tuning hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08f101e-259f-4910-a084-c252ef034dbc",
   "metadata": {},
   "source": [
    "In this article, we'll talk about Population Based Training, explore Ray Tune, and see an example of hyperparameter tunning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c273a66e-bbc3-4edc-9d44-2665e3a020bd",
   "metadata": {},
   "source": [
    "*What PBT means*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3031f788-c8ae-4d2e-9bb2-32925449b97c",
   "metadata": {},
   "source": [
    "As we have already mentioned, the good performance of the model is related to the correct selection of hyperparameters. Population Based Training is one of the charming ways of hyperparameters choosing. It consists of two parts: random search and clever choosing. In the random search step, the algorithm chooses several combinations of hyperparameters randomly. There is a high chance that most of the combinations will have low-performance scores and a small portion of combinations on the contrary will have better / good performance scores. Here comes clever choosing. The second step is in a cycle until we achieve the desired result or we do not exhaust the number of iterations. The clever choosing step contains two main methods: *exploit* and *explore*. *Exploit* - replace the combination of the hyperparameters with more promising ones, based on the performance metric. *Explore* - randomly perturb the hyperparameters (in most cases it is multiplied by some factor) to add noise.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ac15eb-feea-44ef-946b-33564e261400",
   "metadata": {},
   "source": [
    "Population Based Training allows doing two meaningful things together: parallelize training of hyperparameters combinations, study from the rest of the population and get promising results promptly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05bb587-4053-4322-bf08-bb22d667c6bb",
   "metadata": {},
   "source": [
    "*Talk about Tune*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2636b8-2728-400d-8369-62467efd3bd0",
   "metadata": {},
   "source": [
    "Ray Tune is a Ray-based python library for hyperparameter tuning with the latest algorithms such as PBT. We will work on Ray version 2.1.0. Changes can be seen in release notes - https://github.com/ray-project/ray/releases. We will also mention important changes in the way. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088a8dfb-9671-48b3-b700-614996816e09",
   "metadata": {},
   "source": [
    "Before moving on to practical examples let's go over some basic concepts. *Tranable* - is an objective that helps algorithms to evaluate configurations. It can have Class API or Function API, but according to the ray tune documentation, Function API is recommended. *Search Spaces* - values ranges for hyperparameters. *Trials* - Tuner will generate several configurations and run processes on them, so the process runed on a configuration is called Trial. *Search Algorithm* - suggests hyperparameter configurations, by default Tune uses random search as the search algorithm. *Scheduler* - Based on reported results during training process, schedulers decide whether stop or continue. Next meaningful concept is *checkpointing*. Checkpointing means saving intermediate results, necessary to resume and then continue training.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5346f7d3-aba8-4aef-8ab6-2f70a342e5fb",
   "metadata": {},
   "source": [
    "In most cases Search Algorithm and Scheduler can be used together in tuning process, but there is exception. One of the cases, when they are not used together is Population Based Training. In Ray Tune docs PBT is in schedulers part, but it is both at the same time. It's a scheduler because it stops trials based on the results and is a searcher as it has the logic to create a new configuration. Schedulers compatibility can be seen here - https://docs.ray.io/en/latest/tune/key-concepts.html#schedulers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ccac20-8329-425b-ad57-a6d94328271a",
   "metadata": {
    "tags": []
   },
   "source": [
    "We use well known The Boston Housing Dataset (https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html). We can import this dataset from sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2240157c-846f-4240-aee7-11c5703c55da",
   "metadata": {},
   "source": [
    "One meaningful change in Ray Tune was the execution API. tune.run() has changed into Tuner().fit. Before the update, we were passing the parameters separately, but in the new version, config classes were introduced which simplifies a lot of things. First of all, grouped related parameters together which makes execution code easier to read and understand. And second, when you use Ray Tune in a project, some configurations are the same for some algorithms, so you can make one common config class object and move around algorithms, which makes life easier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0c773f-35b3-4a2b-81cf-02ba397b69d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install a pip package\n",
    "import sys\n",
    "!{sys.executable} -m pip install ray==2.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f3468d-d67f-40d5-868f-dd1227c7ecdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import json\n",
    "import os\n",
    "\n",
    "from joblib import dump, load\n",
    "from lightgbm import LGBMRegressor\n",
    "from ray import tune\n",
    "from ray.air.config import CheckpointConfig\n",
    "from ray.air.config import RunConfig\n",
    "from ray.tune.schedulers import PopulationBasedTraining\n",
    "from ray.tune.tune_config import TuneConfig\n",
    "from ray.tune.tuner import Tuner\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ec1fa2-11d4-4fe1-8925-86d60abb32e7",
   "metadata": {},
   "source": [
    "Let's start with trainable. As we already mentioned, there are two trainable APIs: function-based and class-based. We will write trainable with Class API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c0118c-f23b-4828-bcad-d5159d4a6061",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainableForPBT(tune.Trainable):\n",
    "    def setup(self, config):\n",
    "        pass\n",
    "    \n",
    "    def step(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789d2c36-99e4-4917-a9fc-67e0d82aba9a",
   "metadata": {},
   "source": [
    "tune.Trainable is base class for class-based trainables. We need to override at least two methods: <font color=blue>setup</font> and <font color=blue>step</font>. <font color=blue>Setup</font> is invoked when training starts and <font color=blue>step</font> on every iteration. Unlike <font color=blue>setup</font> function, <font color=blue>step</font> can be invoked several times. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b7b249-c634-4a23-98fe-ee80207cf2d5",
   "metadata": {},
   "source": [
    "In the <font color=blue>setup</font> we need to have x_train and y_train to estimate the efficiency of the trial model in future steps. Of course, <font color=blue>setup</font> is the parent class's (tune.Trainable) function, but it gives us the possibility to add additional arguments. Also, we need to initialize the lgbm regressor/model in the <font color=blue>setup</font>. We are going to retrain our model on every iteration, but on the first one we want to just fit the model, hence need to count on which iteration are we in. Nothing more at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443f615b-2b79-4791-9976-00a111b2c65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup(self, config, x_train, y_train):\n",
    "    self.current_config = config\n",
    "    self.x_train = x_train\n",
    "    self.y_train = y_train\n",
    "    # need to add about model\n",
    "    self.model = LGBMRegressor(**self.current_config)\n",
    "    # count on which iteration are we in\n",
    "    self.current_iteration = 0\n",
    "    self.current_step_score = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f1c020-9930-4279-bf78-c26ac63fe5b3",
   "metadata": {},
   "source": [
    "What do we do in <font color=blue>step</font>? We should estimate current configurations efficiency and return score. We will use cross-validation with r2 and return this score. Therefore, PBT will know scores associated with all configurations after each iterations and it will make decisions about perturbation based on these scores. Also we should refit model one more time if it is not first iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c84cb9-3639-44e1-b963-dab48ff3f9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(self):\n",
    "    self.current_iteration += 1\n",
    "    if self.current_iteration == 1:\n",
    "        self.model.fit(self.x_train, self.y_train)\n",
    "    else:\n",
    "        self.model.fit(self.x_train, self.y_train, init_model=self.model)\n",
    "\n",
    "    self.current_step_score = cross_val_score(estimator=self.model, X=self.x_train, y=self.y_train,\n",
    "                                              scoring='r2', cv=5).mean()\n",
    "    results_dict = {\"r2_score\": self.current_step_score}\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede1ec38-45f6-42d7-bc6d-0a6e499585cf",
   "metadata": {},
   "source": [
    "After two main function overrideing PBT needs more functions to override. For the exploition process we need to save and read checkpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31775aac-279f-4741-98c3-5e17dcd0e21b",
   "metadata": {},
   "source": [
    "Start with <font color=blue>save_checkpoint</font>. We will use joblib library for saving and restoring model. What do we need to save? First of all - model, since we always need previous iteration model (init_model) for next iteration, we can also save current iteration number and current step score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6b0e7f-0294-4be3-b723-08d2cd5c906d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(self, tmp_checkpoint_dir):\n",
    "    path = os.path.join(tmp_checkpoint_dir, \"checkpoint\")\n",
    "    with open(path, \"w\") as f:\n",
    "        f.write(json.dumps(\n",
    "            {\"current_score\": self.current_step_score, \"current_step\": self.current_iteration}))\n",
    "\n",
    "    path_for_model = os.path.join(tmp_checkpoint_dir, 'model.joblib')\n",
    "    dump(self.model, path_for_model)\n",
    "\n",
    "    return tmp_checkpoint_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a57ae1-fee1-4644-92e3-36b8e266a87d",
   "metadata": {},
   "source": [
    "We should restore same things from <font color=blue>load_checkpoint</font>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fcda3e-04b4-462a-b29c-8e137ffd86da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(self, tmp_checkpoint_dir):\n",
    "    with open(os.path.join(tmp_checkpoint_dir, \"checkpoint\")) as f:\n",
    "        state = json.loads(f.read())\n",
    "        self.current_step_score = state[\"current_score\"]\n",
    "        self.current_iteration = state[\"current_step\"]\n",
    "\n",
    "    path_for_model = os.path.join(tmp_checkpoint_dir, 'model.joblib')\n",
    "    self.model = load(path_for_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e32e9c-c14c-4754-873f-c81603b20f9d",
   "metadata": {},
   "source": [
    "Above trainable class can be considered as completed. But we can improve the training time with the reuse_actor feature. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6027855d-7292-4c92-8167-207ca9dfa9ff",
   "metadata": {},
   "source": [
    "In the training process, we have as many Trainables as configuration samples. Each Trainable needs several seconds to start. With the reuse_actor feature, we can reuse already started Trainable for new multiple configurations/hyperparameters. So we will need less Trainable and the time spent on the start-up will be less as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78099dd6-fe0f-4c83-9117-2428acb18907",
   "metadata": {},
   "source": [
    "Let's implement <font color=blue>reset_config</font>, which delivers new hyperparameters. In <font color=blue>reset_config</font> every variable need to be adjusted to new hyperparameters, it's like new <font color=blue>setup</font>. There is one tricky question, every time different configurations swap the same Trainable, do they start the process from scratch, due to the fact that in reset_config we write it like the start? Actually, no, because after reset_config, the Trainable calls <font color=blue>load checkpoint</font> if one exists, hence, training will continue from the last stop/checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb6e54e-9ca5-482c-b2a7-61a32c728004",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_config(self, new_config):\n",
    "    self.current_config = new_config\n",
    "    self.current_iteration = 0\n",
    "    self.current_step_score = None\n",
    "    self.model = LGBMRegressor(**self.current_config)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d025844-7a8a-4353-9d40-b6573d5c73b6",
   "metadata": {},
   "source": [
    "We have completed the implementation of Trainable. The finished class will look like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fad10a9-4584-48ae-b3d0-64b06a7053fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainableForPBT(tune.Trainable):\n",
    "    def setup(self, config, x_train, y_train):\n",
    "        self.current_config = config\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        # need to add about model\n",
    "        self.model = LGBMRegressor(**self.current_config)\n",
    "        # count on which iteration are we in\n",
    "        self.current_iteration = 0\n",
    "        self.current_step_score = None\n",
    "\n",
    "    def step(self):\n",
    "        self.current_iteration += 1\n",
    "        if self.current_iteration == 1:\n",
    "            self.model.fit(self.x_train, self.y_train)\n",
    "        else:\n",
    "            self.model.fit(self.x_train, self.y_train, init_model=self.model)\n",
    "\n",
    "        self.current_step_score = cross_val_score(estimator=self.model, X=self.x_train, y=self.y_train,\n",
    "                                                  scoring='r2', cv=5).mean()\n",
    "        results_dict = {\"r2_score\": self.current_step_score}\n",
    "        return results_dict\n",
    "\n",
    "    def save_checkpoint(self, tmp_checkpoint_dir):\n",
    "        path = os.path.join(tmp_checkpoint_dir, \"checkpoint\")\n",
    "        with open(path, \"w\") as f:\n",
    "            f.write(json.dumps(\n",
    "                {\"current_score\": self.current_step_score, \"current_step\": self.current_iteration}))\n",
    "\n",
    "        path_for_model = os.path.join(tmp_checkpoint_dir, 'model.joblib')\n",
    "        dump(self.model, path_for_model)\n",
    "\n",
    "        return tmp_checkpoint_dir\n",
    "\n",
    "    def load_checkpoint(self, tmp_checkpoint_dir):\n",
    "        with open(os.path.join(tmp_checkpoint_dir, \"checkpoint\")) as f:\n",
    "            state = json.loads(f.read())\n",
    "            self.current_step_score = state[\"current_score\"]\n",
    "            self.current_iteration = state[\"current_step\"]\n",
    "\n",
    "        path_for_model = os.path.join(tmp_checkpoint_dir, 'model.joblib')\n",
    "        self.model = load(path_for_model)\n",
    "\n",
    "    def reset_config(self, new_config):\n",
    "        self.current_config = new_config\n",
    "        self.current_iteration = 0\n",
    "        self.current_step_score = None\n",
    "        self.model = LGBMRegressor(**self.current_config)\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcba71da-adf2-4a80-a1e5-2d69de399e61",
   "metadata": {},
   "source": [
    "Now we can create some configurations and run Tune experiment. Tuner has 4 parameter: trainable, param_space, tune_config and run_config. Trainable is already implemented. Let's define param_space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bf6d69-bd7d-4e4f-98e5-8eccd8b2cbba",
   "metadata": {},
   "source": [
    "Param_space is same as already mentioned search space. First, we need to define a list of parameters that we are going to tune. To simplify, choose 3 parameters: learning_rate, num_leaves, max_depth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260f2133-9d90-40d0-9ba9-d93dcf8f819e",
   "metadata": {},
   "source": [
    "Tune has own Search Space API, so we should use them when we define search spaces. The name of search spaces are intuitive, so let's see the result without further ado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af27d17f-d670-4f3b-8aea-e128f90456ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_space = {\n",
    "    \"params\": {\n",
    "        \"learning_rate\": tune.loguniform(1e-5, 1e-1), #between 0.00001 and 0.1\n",
    "        \"num_leaves\":  tune.randint(5, 100), #between 5 and 100(exclusive)\n",
    "        \"max_depth\": tune.randint(1, 9), #between 1 and 9(exclusive)\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970f82a1-75f3-4fd6-98f7-ccc3885e6ed0",
   "metadata": {},
   "source": [
    "Next thing to define is tune_config. But before we implement that, we need to create scheduler - Population Based Training object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c72886-4143-4441-bf56-a9aa7d053ea5",
   "metadata": {},
   "source": [
    "The first parameter of Population Base Training schedule is time_attr. It is the training result attribute for comparison, which should be something that increases monotonically. We choose trainig_iteration as a time attribute, so when we mention time_attr anywhere, that means training_iteration. perturbation_interval - how often should the perturbation occur. If we do perturbation often, then we need to save the checkpoints often as well. Hereby, let's choose perturbation_interval to be 4. burn_in_period - perturbation will not happen before this number of intervals (iteration) has passed. It will not be true if we clone the state of the top performers to poorly performing models from the very beginning, as performance scores are unstable at early stages. So give 10 iterations of burn_in_period trials and then start perturbation. hyperparam_mutations is a dict of hyperparameters and their search spaces, which can be perturbated. We want to perturbate all hyperparameters from the param_space dict, so hyperparam_mutations will be the same as param_space[\"params\"]. We will not pass mode and metric arguments in PopulationBasedTraining, as we define them in TuneConfig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e7e25c-27c1-4db3-8a0f-772a57e1f538",
   "metadata": {},
   "outputs": [],
   "source": [
    "pbt = PopulationBasedTraining(\n",
    "            time_attr=\"training_iteration\",\n",
    "            perturbation_interval=4,\n",
    "            burn_in_period=10,\n",
    "            hyperparam_mutations=param_space[\"params\"],\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef37ffff-f137-4271-99be-752a99d288e9",
   "metadata": {},
   "source": [
    "In TuneConfig we need to pass metric, which is the name of reported score from trainable - \"r2_score\" in our case. Also mode, which has two value min or max, depends on objective minimizing or maximizing the metric. As already mentioned, we don't have a search algorithm and our scheduler algorithm is pbt (PopulationBasedTraining). reuse_actors should be true as well. num_samples - number of samples of hyperparameters from search space should tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008e830c-4b56-45dc-bbbb-247dda926cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_config = TuneConfig(metric=\"r2_score\", mode=\"max\", search_alg=None, scheduler=pbt, num_samples=15, reuse_actors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e341cfc-85ee-419b-b548-60875b4cc724",
   "metadata": {},
   "source": [
    "Next to define is RunConfig, which contains CheckpointConfig inside, so first create CheckpointConfig and then RunConfig. In CheckpointConfig, checkpoint_score_attribute and checkpoint_score_order are the same as metric and mode in TuneConfig. Choose checkpoint_frequency same as perturbation_interval. Also, save the last checkpoint at the end of training with checkpoint_at_end=True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7018f909-ddf2-4a63-be99-d773a8022f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_config = CheckpointConfig(checkpoint_score_attribute=\"r2_score\", \n",
    "                                     checkpoint_score_order=\"max\", \n",
    "                                     checkpoint_frequency=4, \n",
    "                                     checkpoint_at_end=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66873829-f466-4482-9615-079e2bc79423",
   "metadata": {},
   "source": [
    "In Run Config we can pass the name of experment and local_dir, which is directory where training results are saved. It will be useful if we would like to restore/continue the experiment in the future. We should add easy criteria for stopping - stop after 30 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe5f0c8-0237-42ae-b4c6-f4d267d64973",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = RunConfig(name=\"pbt_experiment\", \n",
    "                       local_dir='/Users/admin/Desktop/Dressler/Publications',\n",
    "                       stop={\"training_iteration\": 30},\n",
    "                       checkpoint_config=checkpoint_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0eabd04-f5e1-4634-9ca0-6fc35124a5f0",
   "metadata": {},
   "source": [
    "It's time to create Tuner. Because we have trainable with extra arguments, we need to use and pass tune.with_parameters inside Tuner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6b81bf-3540-46a8-948e-ada390f4941a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_boston(return_X_y=True)\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ba0540-cef9-4771-8b30-5d8d6495e399",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_with_parameters = tune.with_parameters(TrainableForPBT, x_train=x_train, y_train=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9819386f-4f35-42f9-b0aa-1bda4276fc09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tuner = Tuner(trainable_with_parameters, param_space=param_space[\"params\"], tune_config=tune_config, run_config=run_config)\n",
    "analysis = tuner.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d671d83-2a69-489f-991c-9b4e9f8ff9f5",
   "metadata": {},
   "source": [
    "Now, we can interact with results using ResultGrid object (analysis). Using get_best_result we can get best result from the all trials. Also I will show you some useful results from ResultGrid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d8490c-3590-4e53-b0e9-259eb9a51bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial_id = analysis._experiment_analysis.best_trial.trial_id\n",
    "best_result = analysis.get_best_result()\n",
    "best_result_score = best_result.metrics['r2_score']\n",
    "best_config = best_result.config\n",
    "best_checkpoint = best_result.checkpoint\n",
    "best_checkpoint_dir = best_result.checkpoint.as_directory()\n",
    "print(f\"BEST TRIAL ID: {best_trial_id}\")\n",
    "print(f\"BEST RESULT SCORE: {best_result_score}\")\n",
    "print(f\"BEST CHECKPOINT DIRECTORY: {best_checkpoint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435fcec3-7459-4a02-b255-a53c86fef18b",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32918c8f-1f55-4d99-b59e-daf5f2d302ca",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
